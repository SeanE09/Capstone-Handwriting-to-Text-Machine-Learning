{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda activate TFgpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 4.1225 - accuracy: 0.0238 - val_loss: 4.0857 - val_accuracy: 0.0220\n",
      "Epoch 2/5\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 3.7175 - accuracy: 0.0869 - val_loss: 3.2641 - val_accuracy: 0.1745\n",
      "Epoch 3/5\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 2.7254 - accuracy: 0.2819 - val_loss: 2.5832 - val_accuracy: 0.3402\n",
      "Epoch 4/5\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 2.1166 - accuracy: 0.4249 - val_loss: 2.2214 - val_accuracy: 0.4223\n",
      "Epoch 5/5\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.7076 - accuracy: 0.5246 - val_loss: 1.8346 - val_accuracy: 0.5337\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fe5fe1ff10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import cv2\n",
    "# import os\n",
    "# import numpy as np\n",
    "# from PIL import Image\n",
    "# import pandas as pd\n",
    "# from sklearn import preprocessing\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\n",
    "# import glob\n",
    "\n",
    "# # Get the current directory\n",
    "# current_directory = os.getcwd()\n",
    "\n",
    "# # Specify the folder name where the images are located\n",
    "# folder_name = \"img\"\n",
    "\n",
    "# # Construct the folder path\n",
    "# folder_path = os.path.join(current_directory, folder_name)\n",
    "\n",
    "# # Specify the CSV file path containing image labels\n",
    "# csv_file_path = os.path.join(current_directory, \"labels\", \"english-clean.csv\")\n",
    "\n",
    "# # Read the CSV file into a DataFrame\n",
    "# df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# # Create a dictionary where keys are filenames and values are labels\n",
    "# labels_dict = df.set_index('image')['label'].to_dict()\n",
    "\n",
    "# def load_images_from_folder(folder, labels_dict):\n",
    "#     images = []\n",
    "#     labels = []\n",
    "#     for file_path in glob.glob(os.path.join(folder, \"*.png\")):\n",
    "#         img = Image.open(file_path)\n",
    "#         if img is not None:\n",
    "#             img = img.convert('L')  # Convert image to grayscale\n",
    "#             img = img.resize((28, 28))  # Resize the image\n",
    "#             np_img = np.array(img)\n",
    "#             images.append(np_img)\n",
    "#             filename = os.path.basename(file_path)\n",
    "#             label = labels_dict[filename]  # Get the label from the filename\n",
    "#             labels.append(label)\n",
    "#     return images, labels\n",
    "\n",
    "# images, labels = load_images_from_folder(folder_path, labels_dict)\n",
    "\n",
    "# le = preprocessing.LabelEncoder()\n",
    "# numerical_labels = le.fit_transform(labels)  # Convert labels to integers\n",
    "\n",
    "# images = np.array(images)  # Convert list of arrays to a single array\n",
    "# images = images / 255.0  # Normalize pixel values\n",
    "# images = images.reshape(-1, 28, 28, 1)  # Reshape array for CNN\n",
    "\n",
    "# encoded_labels = to_categorical(numerical_labels)  # One-hot encode labels\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(images, encoded_labels, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "# model.add(MaxPooling2D((2, 2)))\n",
    "# model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "# model.add(MaxPooling2D((2, 2)))\n",
    "# model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(64, activation='relu'))\n",
    "# num_classes = len(set(numerical_labels))\n",
    "# model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss='categorical_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# model.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 4.0487 - accuracy: 0.0286 - val_loss: 3.7491 - val_accuracy: 0.0865\n",
      "Epoch 2/40\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 3.1456 - accuracy: 0.1954 - val_loss: 2.7184 - val_accuracy: 0.2742\n",
      "Epoch 3/40\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 2.0651 - accuracy: 0.4395 - val_loss: 1.8162 - val_accuracy: 0.4985\n",
      "Epoch 4/40\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 1.3487 - accuracy: 0.6045 - val_loss: 1.4531 - val_accuracy: 0.5704\n",
      "Epoch 5/40\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 1.0118 - accuracy: 0.7023 - val_loss: 1.2266 - val_accuracy: 0.6598\n",
      "Epoch 6/40\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.7421 - accuracy: 0.7639 - val_loss: 1.1699 - val_accuracy: 0.6657\n",
      "Epoch 7/40\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.5630 - accuracy: 0.8098 - val_loss: 1.1438 - val_accuracy: 0.6642\n",
      "Epoch 8/40\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.4557 - accuracy: 0.8471 - val_loss: 1.1156 - val_accuracy: 0.6891\n",
      "Epoch 9/40\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.3542 - accuracy: 0.8809 - val_loss: 1.2289 - val_accuracy: 0.6716\n",
      "Epoch 10/40\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2931 - accuracy: 0.9095 - val_loss: 1.2226 - val_accuracy: 0.7009\n",
      "Epoch 11/40\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2523 - accuracy: 0.9135 - val_loss: 1.2302 - val_accuracy: 0.7273\n",
      "Epoch 12/40\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2076 - accuracy: 0.9300 - val_loss: 1.2205 - val_accuracy: 0.7155\n",
      "Epoch 13/40\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.1708 - accuracy: 0.9457 - val_loss: 1.2775 - val_accuracy: 0.7185\n",
      "Epoch 14/40\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.1498 - accuracy: 0.9520 - val_loss: 1.4261 - val_accuracy: 0.7009\n",
      "Epoch 15/40\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.1316 - accuracy: 0.9593 - val_loss: 1.2979 - val_accuracy: 0.7243\n",
      "Epoch 16/40\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.1042 - accuracy: 0.9619 - val_loss: 1.5053 - val_accuracy: 0.6950\n",
      "Epoch 17/40\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.1266 - accuracy: 0.9560 - val_loss: 1.4724 - val_accuracy: 0.7141\n",
      "Epoch 18/40\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.1358 - accuracy: 0.9542 - val_loss: 1.5958 - val_accuracy: 0.7053\n",
      "Epoch 19/40\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.0950 - accuracy: 0.9718 - val_loss: 1.6397 - val_accuracy: 0.7170\n",
      "Epoch 20/40\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.1186 - accuracy: 0.9622 - val_loss: 1.4938 - val_accuracy: 0.6994\n",
      "Epoch 21/40\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.0910 - accuracy: 0.9699 - val_loss: 1.5793 - val_accuracy: 0.7141\n",
      "Epoch 22/40\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.0464 - accuracy: 0.9850 - val_loss: 1.6357 - val_accuracy: 0.7214\n",
      "Epoch 23/40\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.0304 - accuracy: 0.9886 - val_loss: 1.7411 - val_accuracy: 0.7317\n",
      "Epoch 24/40\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.0337 - accuracy: 0.9883 - val_loss: 1.8063 - val_accuracy: 0.7067\n",
      "Epoch 25/40\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.0944 - accuracy: 0.9670 - val_loss: 1.9085 - val_accuracy: 0.6862\n",
      "Epoch 26/40\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.1552 - accuracy: 0.9509 - val_loss: 1.6398 - val_accuracy: 0.7126\n",
      "Epoch 27/40\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.0711 - accuracy: 0.9765 - val_loss: 1.5142 - val_accuracy: 0.7199\n",
      "Epoch 28/40\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.1338 - accuracy: 0.9604 - val_loss: 1.5007 - val_accuracy: 0.7067\n",
      "Epoch 29/40\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.0739 - accuracy: 0.9791 - val_loss: 1.6710 - val_accuracy: 0.7097\n",
      "Epoch 30/40\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.0664 - accuracy: 0.9791 - val_loss: 1.7855 - val_accuracy: 0.7170\n",
      "Epoch 31/40\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.0480 - accuracy: 0.9846 - val_loss: 1.7426 - val_accuracy: 0.7155\n",
      "Epoch 32/40\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.0301 - accuracy: 0.9905 - val_loss: 2.0017 - val_accuracy: 0.6950\n",
      "Epoch 33/40\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.0543 - accuracy: 0.9824 - val_loss: 2.0807 - val_accuracy: 0.6906\n",
      "Epoch 34/40\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.1379 - accuracy: 0.9542 - val_loss: 1.6631 - val_accuracy: 0.6965\n",
      "Epoch 35/40\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.1173 - accuracy: 0.9600 - val_loss: 1.7034 - val_accuracy: 0.7199\n",
      "Epoch 36/40\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.0282 - accuracy: 0.9901 - val_loss: 1.8108 - val_accuracy: 0.7141\n",
      "Epoch 37/40\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 0.0415 - accuracy: 0.9879 - val_loss: 1.7129 - val_accuracy: 0.7243\n",
      "Epoch 38/40\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.0221 - accuracy: 0.9945 - val_loss: 1.6972 - val_accuracy: 0.7361\n",
      "Epoch 39/40\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.0101 - accuracy: 0.9956 - val_loss: 1.7399 - val_accuracy: 0.7419\n",
      "Epoch 40/40\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.0066 - accuracy: 0.9974 - val_loss: 1.7976 - val_accuracy: 0.7375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fe60969750>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import cv2\n",
    "# import os\n",
    "# import numpy as np\n",
    "# from PIL import Image\n",
    "# import pandas as pd\n",
    "# from sklearn import preprocessing\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\n",
    "# import glob\n",
    "\n",
    "# # Get the current directory\n",
    "# current_directory = os.getcwd()\n",
    "\n",
    "# # Specify the folder name where the images are located\n",
    "# folder_name = \"img\"\n",
    "\n",
    "# # Construct the folder path\n",
    "# folder_path = os.path.join(current_directory, folder_name)\n",
    "\n",
    "# # Specify the CSV file path containing image labels\n",
    "# csv_file_path = os.path.join(current_directory, \"labels\", \"english-clean.csv\")\n",
    "\n",
    "# # Read the CSV file into a DataFrame\n",
    "# df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# # Create a dictionary where keys are filenames and values are labels\n",
    "# labels_dict = df.set_index('image')['label'].to_dict()\n",
    "\n",
    "# def load_images_from_folder(folder, labels_dict):\n",
    "#     images = []\n",
    "#     labels = []\n",
    "#     for file_path in glob.glob(os.path.join(folder, \"*.png\")):\n",
    "#         img = Image.open(file_path)\n",
    "#         if img is not None:\n",
    "#             img = img.convert('L')  # Convert image to grayscale\n",
    "#             img = img.resize((28, 28))  # Resize the image\n",
    "#             np_img = np.array(img)\n",
    "#             images.append(np_img)\n",
    "#             filename = os.path.basename(file_path)\n",
    "#             label = labels_dict[filename]  # Get the label from the filename\n",
    "#             labels.append(label)\n",
    "#     return images, labels\n",
    "\n",
    "# images, labels = load_images_from_folder(folder_path, labels_dict)\n",
    "\n",
    "# le = preprocessing.LabelEncoder()\n",
    "# numerical_labels = le.fit_transform(labels)  # Convert labels to integers\n",
    "\n",
    "# images = np.array(images)  # Convert list of arrays to a single array\n",
    "# images = images / 255.0  # Normalize pixel values\n",
    "# images = images.reshape(-1, 28, 28, 1)  # Reshape array for CNN\n",
    "\n",
    "# encoded_labels = to_categorical(numerical_labels)  # One-hot encode labels\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(images, encoded_labels, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# model = Sequential()\n",
    "\n",
    "# model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "# model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "# model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "# model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "# model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(Flatten())\n",
    "\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "\n",
    "# num_classes = len(set(numerical_labels))\n",
    "# model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss='categorical_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# model.fit(X_train, y_train, epochs=40, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 4.1274 - accuracy: 0.0136 - val_loss: 4.0938 - val_accuracy: 0.0308\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 3.3595 - accuracy: 0.1540 - val_loss: 2.5466 - val_accuracy: 0.3138\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 2.0615 - accuracy: 0.4238 - val_loss: 1.9384 - val_accuracy: 0.4487\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 1.4710 - accuracy: 0.5579 - val_loss: 1.4671 - val_accuracy: 0.5777\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 1.0963 - accuracy: 0.6510 - val_loss: 1.3418 - val_accuracy: 0.6158\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.8966 - accuracy: 0.7060 - val_loss: 1.2017 - val_accuracy: 0.6554\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.7492 - accuracy: 0.7430 - val_loss: 1.1940 - val_accuracy: 0.6452\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.6301 - accuracy: 0.7808 - val_loss: 1.1208 - val_accuracy: 0.6730\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.5240 - accuracy: 0.8145 - val_loss: 1.1526 - val_accuracy: 0.6789\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.4369 - accuracy: 0.8526 - val_loss: 1.1824 - val_accuracy: 0.6730\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.3629 - accuracy: 0.8702 - val_loss: 1.1361 - val_accuracy: 0.6848\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.3358 - accuracy: 0.8798 - val_loss: 1.4035 - val_accuracy: 0.6276\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.3464 - accuracy: 0.8765 - val_loss: 1.2415 - val_accuracy: 0.6804\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2902 - accuracy: 0.9032 - val_loss: 1.2217 - val_accuracy: 0.7009\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2178 - accuracy: 0.9216 - val_loss: 1.3744 - val_accuracy: 0.6848\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2123 - accuracy: 0.9274 - val_loss: 1.3224 - val_accuracy: 0.7170\n",
      "Epoch 17/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.1812 - accuracy: 0.9348 - val_loss: 1.4251 - val_accuracy: 0.6979\n",
      "Epoch 18/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.1906 - accuracy: 0.9337 - val_loss: 1.5050 - val_accuracy: 0.6818\n",
      "Epoch 19/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.1829 - accuracy: 0.9428 - val_loss: 1.5636 - val_accuracy: 0.7038\n",
      "Epoch 20/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.1459 - accuracy: 0.9472 - val_loss: 1.5835 - val_accuracy: 0.6950\n",
      "Epoch 21/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.1553 - accuracy: 0.9457 - val_loss: 1.4926 - val_accuracy: 0.7141\n",
      "Epoch 22/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.1423 - accuracy: 0.9531 - val_loss: 1.3987 - val_accuracy: 0.7111\n",
      "Epoch 23/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.1086 - accuracy: 0.9630 - val_loss: 1.3130 - val_accuracy: 0.7258\n",
      "Epoch 24/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.0972 - accuracy: 0.9641 - val_loss: 1.5521 - val_accuracy: 0.7038\n",
      "Epoch 25/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.0844 - accuracy: 0.9670 - val_loss: 1.5612 - val_accuracy: 0.7141\n",
      "Epoch 26/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.0677 - accuracy: 0.9747 - val_loss: 1.7567 - val_accuracy: 0.7185\n",
      "Epoch 27/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.1116 - accuracy: 0.9619 - val_loss: 1.7560 - val_accuracy: 0.7023\n",
      "Epoch 28/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.1548 - accuracy: 0.9443 - val_loss: 1.4889 - val_accuracy: 0.7258\n",
      "Epoch 29/100\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.0809 - accuracy: 0.9696 - val_loss: 1.6505 - val_accuracy: 0.6965\n",
      "Epoch 30/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.1386 - accuracy: 0.9571 - val_loss: 1.5890 - val_accuracy: 0.7141\n",
      "Epoch 31/100\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.0605 - accuracy: 0.9809 - val_loss: 1.6498 - val_accuracy: 0.7023\n",
      "Epoch 32/100\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.0317 - accuracy: 0.9890 - val_loss: 1.8049 - val_accuracy: 0.7038\n",
      "Epoch 33/100\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.0305 - accuracy: 0.9908 - val_loss: 1.8414 - val_accuracy: 0.7097\n",
      "Epoch 34/100\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.0300 - accuracy: 0.9916 - val_loss: 1.7348 - val_accuracy: 0.7199\n",
      "Epoch 35/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.0188 - accuracy: 0.9916 - val_loss: 1.8974 - val_accuracy: 0.6979\n",
      "Epoch 36/100\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.1792 - accuracy: 0.9428 - val_loss: 1.7727 - val_accuracy: 0.6672\n",
      "Epoch 37/100\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.0990 - accuracy: 0.9655 - val_loss: 1.7456 - val_accuracy: 0.6979\n",
      "Epoch 38/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.0883 - accuracy: 0.9740 - val_loss: 1.6653 - val_accuracy: 0.7038\n",
      "Epoch 39/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.0774 - accuracy: 0.9758 - val_loss: 1.6805 - val_accuracy: 0.7141\n",
      "Epoch 40/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.0829 - accuracy: 0.9751 - val_loss: 1.8103 - val_accuracy: 0.7067\n",
      "Epoch 41/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.1181 - accuracy: 0.9630 - val_loss: 1.8671 - val_accuracy: 0.6979\n",
      "Epoch 42/100\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.0914 - accuracy: 0.9648 - val_loss: 1.5858 - val_accuracy: 0.7302\n",
      "Epoch 43/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.0869 - accuracy: 0.9699 - val_loss: 1.5365 - val_accuracy: 0.7097\n",
      "Epoch 44/100\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.0593 - accuracy: 0.9817 - val_loss: 1.7920 - val_accuracy: 0.7067\n",
      "Epoch 45/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.0920 - accuracy: 0.9677 - val_loss: 1.7514 - val_accuracy: 0.6979\n",
      "Epoch 46/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.0434 - accuracy: 0.9853 - val_loss: 1.8501 - val_accuracy: 0.7214\n",
      "Epoch 47/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.0600 - accuracy: 0.9809 - val_loss: 1.8898 - val_accuracy: 0.7067\n",
      "Epoch 48/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.0676 - accuracy: 0.9820 - val_loss: 1.8923 - val_accuracy: 0.6965\n",
      "Epoch 49/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.0545 - accuracy: 0.9828 - val_loss: 1.8992 - val_accuracy: 0.7302\n",
      "Epoch 50/100\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.0263 - accuracy: 0.9923 - val_loss: 1.8693 - val_accuracy: 0.7170\n",
      "Epoch 51/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.0093 - accuracy: 0.9971 - val_loss: 1.9630 - val_accuracy: 0.7155\n",
      "Epoch 52/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.0108 - accuracy: 0.9963 - val_loss: 1.9853 - val_accuracy: 0.7155\n",
      "Epoch 53/100\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.0065 - accuracy: 0.9982 - val_loss: 2.0432 - val_accuracy: 0.7170\n",
      "Epoch 54/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.0058 - accuracy: 0.9974 - val_loss: 2.1501 - val_accuracy: 0.7243\n",
      "Epoch 55/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.0107 - accuracy: 0.9952 - val_loss: 2.1320 - val_accuracy: 0.7258\n",
      "Epoch 56/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.0106 - accuracy: 0.9963 - val_loss: 2.1222 - val_accuracy: 0.7199\n",
      "Epoch 57/100\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.0040 - accuracy: 0.9989 - val_loss: 2.1450 - val_accuracy: 0.7273\n",
      "Epoch 58/100\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.0050 - accuracy: 0.9978 - val_loss: 2.1545 - val_accuracy: 0.7302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.1855 - accuracy: 0.9443 - val_loss: 1.7042 - val_accuracy: 0.6730\n",
      "Epoch 60/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.1404 - accuracy: 0.9527 - val_loss: 1.8738 - val_accuracy: 0.6921\n",
      "Epoch 61/100\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.0854 - accuracy: 0.9721 - val_loss: 1.8556 - val_accuracy: 0.6877\n",
      "Epoch 62/100\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.0724 - accuracy: 0.9758 - val_loss: 1.7378 - val_accuracy: 0.6994\n",
      "Epoch 63/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.0997 - accuracy: 0.9710 - val_loss: 1.7096 - val_accuracy: 0.6950\n",
      "Epoch 64/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.0525 - accuracy: 0.9842 - val_loss: 1.9746 - val_accuracy: 0.7067\n",
      "Epoch 65/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.0538 - accuracy: 0.9828 - val_loss: 1.9891 - val_accuracy: 0.7185\n",
      "Epoch 66/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.0755 - accuracy: 0.9718 - val_loss: 1.8530 - val_accuracy: 0.6848\n",
      "Epoch 67/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.0822 - accuracy: 0.9769 - val_loss: 1.7402 - val_accuracy: 0.7185\n",
      "Epoch 68/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.0206 - accuracy: 0.9927 - val_loss: 1.7632 - val_accuracy: 0.7419\n",
      "Epoch 69/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.0220 - accuracy: 0.9938 - val_loss: 2.0232 - val_accuracy: 0.7141\n",
      "Epoch 70/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.0084 - accuracy: 0.9978 - val_loss: 2.0112 - val_accuracy: 0.7141\n",
      "Epoch 71/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.0125 - accuracy: 0.9974 - val_loss: 2.1307 - val_accuracy: 0.7199\n",
      "Epoch 72/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.0115 - accuracy: 0.9963 - val_loss: 2.0702 - val_accuracy: 0.7141\n",
      "Epoch 73/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.0518 - accuracy: 0.9872 - val_loss: 1.8720 - val_accuracy: 0.7141\n",
      "Epoch 74/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.0634 - accuracy: 0.9802 - val_loss: 1.9076 - val_accuracy: 0.7170\n",
      "Epoch 75/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.1212 - accuracy: 0.9611 - val_loss: 1.7632 - val_accuracy: 0.7009\n",
      "Epoch 76/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.0659 - accuracy: 0.9795 - val_loss: 1.8423 - val_accuracy: 0.7199\n",
      "Epoch 77/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.0329 - accuracy: 0.9916 - val_loss: 1.8091 - val_accuracy: 0.7287\n",
      "Epoch 78/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.0256 - accuracy: 0.9930 - val_loss: 2.0870 - val_accuracy: 0.7009\n",
      "Epoch 79/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.0539 - accuracy: 0.9824 - val_loss: 2.0923 - val_accuracy: 0.7111\n",
      "Epoch 80/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.1302 - accuracy: 0.9586 - val_loss: 1.6751 - val_accuracy: 0.7273\n",
      "Epoch 81/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.0554 - accuracy: 0.9842 - val_loss: 1.9310 - val_accuracy: 0.7155\n",
      "Epoch 82/100\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.0185 - accuracy: 0.9963 - val_loss: 1.9777 - val_accuracy: 0.7097\n",
      "Epoch 83/100\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.0329 - accuracy: 0.9908 - val_loss: 2.1591 - val_accuracy: 0.7023\n",
      "Epoch 84/100\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.0146 - accuracy: 0.9952 - val_loss: 2.1083 - val_accuracy: 0.7258\n",
      "Epoch 85/100\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.0044 - accuracy: 0.9989 - val_loss: 2.2308 - val_accuracy: 0.7273\n",
      "Epoch 86/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.0042 - accuracy: 0.9982 - val_loss: 2.2879 - val_accuracy: 0.7258\n",
      "Epoch 87/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.0040 - accuracy: 0.9985 - val_loss: 2.2318 - val_accuracy: 0.7243\n",
      "Epoch 88/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.0057 - accuracy: 0.9982 - val_loss: 2.2674 - val_accuracy: 0.7214\n",
      "Epoch 89/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.0091 - accuracy: 0.9963 - val_loss: 2.2964 - val_accuracy: 0.7273\n",
      "Epoch 90/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.0647 - accuracy: 0.9835 - val_loss: 1.9730 - val_accuracy: 0.6950\n",
      "Epoch 91/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.1188 - accuracy: 0.9666 - val_loss: 1.8248 - val_accuracy: 0.7038\n",
      "Epoch 92/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.0839 - accuracy: 0.9762 - val_loss: 1.8740 - val_accuracy: 0.6891\n",
      "Epoch 93/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.0561 - accuracy: 0.9809 - val_loss: 1.9498 - val_accuracy: 0.7097\n",
      "Epoch 94/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.0425 - accuracy: 0.9868 - val_loss: 2.0502 - val_accuracy: 0.6965\n",
      "Epoch 95/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.0305 - accuracy: 0.9908 - val_loss: 2.0890 - val_accuracy: 0.7023\n",
      "Epoch 96/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.0361 - accuracy: 0.9872 - val_loss: 2.3668 - val_accuracy: 0.7038\n",
      "Epoch 97/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.0678 - accuracy: 0.9773 - val_loss: 2.2738 - val_accuracy: 0.6657\n",
      "Epoch 98/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.0591 - accuracy: 0.9846 - val_loss: 1.8840 - val_accuracy: 0.7243\n",
      "Epoch 99/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.0193 - accuracy: 0.9941 - val_loss: 2.0310 - val_accuracy: 0.7097\n",
      "Epoch 100/100\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.0041 - accuracy: 0.9993 - val_loss: 2.0927 - val_accuracy: 0.7170\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fe61b5d9c0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import cv2\n",
    "# import os\n",
    "# import numpy as np\n",
    "# from PIL import Image\n",
    "# import pandas as pd\n",
    "# from sklearn import preprocessing\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\n",
    "# import glob\n",
    "\n",
    "# # Get the current directory\n",
    "# current_directory = os.getcwd()\n",
    "\n",
    "# # Specify the folder name where the images are located\n",
    "# folder_name = \"img\"\n",
    "\n",
    "# # Construct the folder path\n",
    "# folder_path = os.path.join(current_directory, folder_name)\n",
    "\n",
    "# # Specify the CSV file path containing image labels\n",
    "# csv_file_path = os.path.join(current_directory, \"labels\", \"english-clean.csv\")\n",
    "\n",
    "# # Read the CSV file into a DataFrame\n",
    "# df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# # Create a dictionary where keys are filenames and values are labels\n",
    "# labels_dict = df.set_index('image')['label'].to_dict()\n",
    "\n",
    "# def load_images_from_folder(folder, labels_dict):\n",
    "#     images = []\n",
    "#     labels = []\n",
    "#     for file_path in glob.glob(os.path.join(folder, \"*.png\")):\n",
    "#         img = Image.open(file_path)\n",
    "#         if img is not None:\n",
    "#             img = img.convert('L')  # Convert image to grayscale\n",
    "#             img = img.resize((28, 28))  # Resize the image\n",
    "#             np_img = np.array(img)\n",
    "#             images.append(np_img)\n",
    "#             filename = os.path.basename(file_path)\n",
    "#             label = labels_dict[filename]  # Get the label from the filename\n",
    "#             labels.append(label)\n",
    "#     return images, labels\n",
    "\n",
    "# images, labels = load_images_from_folder(folder_path, labels_dict)\n",
    "\n",
    "# le = preprocessing.LabelEncoder()\n",
    "# numerical_labels = le.fit_transform(labels)  # Convert labels to integers\n",
    "\n",
    "# images = np.array(images)  # Convert list of arrays to a single array\n",
    "# images = images / 255.0  # Normalize pixel values\n",
    "# images = images.reshape(-1, 28, 28, 1)  # Reshape array for CNN\n",
    "\n",
    "# encoded_labels = to_categorical(numerical_labels)  # One-hot encode labels\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(images, encoded_labels, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# model = Sequential()\n",
    "\n",
    "# model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "# model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "# model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "# model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "# model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(Flatten())\n",
    "\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "\n",
    "# model.add(Dense(228, activation='relu'))\n",
    "\n",
    "# num_classes = len(set(numerical_labels))\n",
    "# model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss='categorical_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "86/86 [==============================] - 3s 25ms/step - loss: 4.5012 - accuracy: 0.0554 - val_loss: 11.9708 - val_accuracy: 0.0117\n",
      "Epoch 2/300\n",
      "86/86 [==============================] - 1s 15ms/step - loss: 3.6231 - accuracy: 0.1221 - val_loss: 4.2682 - val_accuracy: 0.0440\n",
      "Epoch 3/300\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 3.1685 - accuracy: 0.1990 - val_loss: 3.4369 - val_accuracy: 0.1320\n",
      "Epoch 4/300\n",
      "86/86 [==============================] - 3s 36ms/step - loss: 2.7277 - accuracy: 0.2793 - val_loss: 2.5696 - val_accuracy: 0.2551\n",
      "Epoch 5/300\n",
      "86/86 [==============================] - 7s 78ms/step - loss: 2.3888 - accuracy: 0.3614 - val_loss: 1.9015 - val_accuracy: 0.4809\n",
      "Epoch 6/300\n",
      "86/86 [==============================] - 4s 49ms/step - loss: 2.2130 - accuracy: 0.3915 - val_loss: 2.0125 - val_accuracy: 0.4472\n",
      "Epoch 7/300\n",
      "86/86 [==============================] - 4s 49ms/step - loss: 2.0280 - accuracy: 0.4311 - val_loss: 3.3763 - val_accuracy: 0.2683\n",
      "Epoch 8/300\n",
      "86/86 [==============================] - 4s 48ms/step - loss: 1.8887 - accuracy: 0.4608 - val_loss: 1.3032 - val_accuracy: 0.6129\n",
      "Epoch 9/300\n",
      "86/86 [==============================] - 6s 63ms/step - loss: 1.7576 - accuracy: 0.4842 - val_loss: 3.1191 - val_accuracy: 0.3152\n",
      "Epoch 10/300\n",
      "86/86 [==============================] - 4s 49ms/step - loss: 1.6453 - accuracy: 0.5136 - val_loss: 1.0945 - val_accuracy: 0.6774\n",
      "Epoch 11/300\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 1.5490 - accuracy: 0.5374 - val_loss: 1.2588 - val_accuracy: 0.6100\n",
      "Epoch 12/300\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 1.4818 - accuracy: 0.5557 - val_loss: 2.4061 - val_accuracy: 0.4150\n",
      "Epoch 13/300\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 1.4397 - accuracy: 0.5623 - val_loss: 5.0647 - val_accuracy: 0.2361\n",
      "Epoch 14/300\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 1.3703 - accuracy: 0.5872 - val_loss: 2.1998 - val_accuracy: 0.4633\n",
      "Epoch 15/300\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 1.3036 - accuracy: 0.5920 - val_loss: 1.4329 - val_accuracy: 0.5982\n",
      "Epoch 16/300\n",
      "86/86 [==============================] - 1s 17ms/step - loss: 1.2568 - accuracy: 0.6056 - val_loss: 0.9714 - val_accuracy: 0.6950\n",
      "Epoch 17/300\n",
      "86/86 [==============================] - 1s 16ms/step - loss: 1.1964 - accuracy: 0.6353 - val_loss: 0.7704 - val_accuracy: 0.7566\n",
      "Epoch 18/300\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 1.1536 - accuracy: 0.6345 - val_loss: 1.2274 - val_accuracy: 0.6408\n",
      "Epoch 19/300\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 1.1517 - accuracy: 0.6356 - val_loss: 2.0454 - val_accuracy: 0.5029\n",
      "Epoch 20/300\n",
      "86/86 [==============================] - 1s 15ms/step - loss: 1.1109 - accuracy: 0.6558 - val_loss: 1.6173 - val_accuracy: 0.5645\n",
      "Epoch 21/300\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 1.0768 - accuracy: 0.6664 - val_loss: 0.8172 - val_accuracy: 0.7214\n",
      "Epoch 22/300\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 1.0619 - accuracy: 0.6719 - val_loss: 0.9218 - val_accuracy: 0.7258\n",
      "Epoch 23/300\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 1.0359 - accuracy: 0.6657 - val_loss: 1.7112 - val_accuracy: 0.5161\n",
      "Epoch 24/300\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 1.0017 - accuracy: 0.6848 - val_loss: 0.6166 - val_accuracy: 0.8079\n",
      "Epoch 25/300\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 1.0213 - accuracy: 0.6822 - val_loss: 0.6866 - val_accuracy: 0.7874\n",
      "Epoch 26/300\n",
      "86/86 [==============================] - 1s 15ms/step - loss: 0.9776 - accuracy: 0.6917 - val_loss: 1.1741 - val_accuracy: 0.6422\n",
      "Epoch 27/300\n",
      "86/86 [==============================] - 2s 22ms/step - loss: 0.9413 - accuracy: 0.7031 - val_loss: 0.6183 - val_accuracy: 0.8109\n",
      "Epoch 28/300\n",
      "86/86 [==============================] - 1s 15ms/step - loss: 0.9151 - accuracy: 0.6961 - val_loss: 1.1184 - val_accuracy: 0.6804\n",
      "Epoch 29/300\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.9208 - accuracy: 0.7005 - val_loss: 0.8101 - val_accuracy: 0.7463\n",
      "Epoch 30/300\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.9168 - accuracy: 0.7078 - val_loss: 0.9382 - val_accuracy: 0.7053\n",
      "Epoch 31/300\n",
      "86/86 [==============================] - 1s 15ms/step - loss: 0.8546 - accuracy: 0.7298 - val_loss: 0.7619 - val_accuracy: 0.7463\n",
      "Epoch 32/300\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 0.8526 - accuracy: 0.7207 - val_loss: 0.7686 - val_accuracy: 0.7478\n",
      "Epoch 33/300\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.8733 - accuracy: 0.7174 - val_loss: 0.9690 - val_accuracy: 0.7126\n",
      "Epoch 34/300\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.8807 - accuracy: 0.7141 - val_loss: 0.6978 - val_accuracy: 0.7889\n",
      "Epoch 35/300\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.8443 - accuracy: 0.7258 - val_loss: 0.8456 - val_accuracy: 0.7375\n",
      "Epoch 36/300\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.8321 - accuracy: 0.7221 - val_loss: 0.5135 - val_accuracy: 0.8358\n",
      "Epoch 37/300\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.7718 - accuracy: 0.7533 - val_loss: 0.9837 - val_accuracy: 0.7155\n",
      "Epoch 38/300\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.8088 - accuracy: 0.7390 - val_loss: 0.9261 - val_accuracy: 0.7405\n",
      "Epoch 39/300\n",
      "86/86 [==============================] - 2s 17ms/step - loss: 0.8201 - accuracy: 0.7298 - val_loss: 0.8942 - val_accuracy: 0.7287\n",
      "Epoch 40/300\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 0.7763 - accuracy: 0.7471 - val_loss: 0.6053 - val_accuracy: 0.7991\n",
      "Epoch 41/300\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.7462 - accuracy: 0.7511 - val_loss: 1.6548 - val_accuracy: 0.5909\n",
      "Epoch 42/300\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.7507 - accuracy: 0.7551 - val_loss: 0.4702 - val_accuracy: 0.8328\n",
      "Epoch 43/300\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 0.7652 - accuracy: 0.7562 - val_loss: 0.8565 - val_accuracy: 0.7199\n",
      "Epoch 44/300\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.7747 - accuracy: 0.7504 - val_loss: 0.5880 - val_accuracy: 0.8109\n",
      "Epoch 45/300\n",
      "86/86 [==============================] - 1s 15ms/step - loss: 0.7957 - accuracy: 0.7423 - val_loss: 0.6133 - val_accuracy: 0.7977\n",
      "Epoch 46/300\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.7166 - accuracy: 0.7562 - val_loss: 0.7788 - val_accuracy: 0.7639\n",
      "Epoch 47/300\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.7001 - accuracy: 0.7683 - val_loss: 0.6621 - val_accuracy: 0.7815\n",
      "Epoch 48/300\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.7246 - accuracy: 0.7588 - val_loss: 0.6548 - val_accuracy: 0.7786\n",
      "Epoch 49/300\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.7134 - accuracy: 0.7628 - val_loss: 0.6236 - val_accuracy: 0.7830\n",
      "Epoch 50/300\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.7116 - accuracy: 0.7584 - val_loss: 0.5238 - val_accuracy: 0.8255\n",
      "Epoch 51/300\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.7301 - accuracy: 0.7595 - val_loss: 0.7761 - val_accuracy: 0.7683\n",
      "Epoch 52/300\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.7386 - accuracy: 0.7603 - val_loss: 0.5818 - val_accuracy: 0.8050\n",
      "Epoch 53/300\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.6803 - accuracy: 0.7694 - val_loss: 0.6226 - val_accuracy: 0.7801\n",
      "Epoch 54/300\n",
      "86/86 [==============================] - 1s 16ms/step - loss: 0.6866 - accuracy: 0.7661 - val_loss: 0.4376 - val_accuracy: 0.8504\n",
      "Epoch 55/300\n",
      "86/86 [==============================] - 1s 15ms/step - loss: 0.6465 - accuracy: 0.7896 - val_loss: 0.4582 - val_accuracy: 0.8460\n",
      "Epoch 56/300\n",
      "86/86 [==============================] - 1s 15ms/step - loss: 0.6718 - accuracy: 0.7764 - val_loss: 0.5803 - val_accuracy: 0.8065\n",
      "Epoch 57/300\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 0.6460 - accuracy: 0.7845 - val_loss: 0.4864 - val_accuracy: 0.8402\n",
      "Epoch 58/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 1s 14ms/step - loss: 0.6475 - accuracy: 0.7852 - val_loss: 0.4851 - val_accuracy: 0.8314\n",
      "Epoch 59/300\n",
      "86/86 [==============================] - 1s 16ms/step - loss: 0.6256 - accuracy: 0.7881 - val_loss: 0.5312 - val_accuracy: 0.8196\n",
      "Epoch 60/300\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.6455 - accuracy: 0.7845 - val_loss: 0.4474 - val_accuracy: 0.8578\n",
      "Epoch 61/300\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.5810 - accuracy: 0.8039 - val_loss: 1.1377 - val_accuracy: 0.6833\n",
      "Epoch 62/300\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.6414 - accuracy: 0.7914 - val_loss: 0.6160 - val_accuracy: 0.8006\n",
      "Epoch 63/300\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.6413 - accuracy: 0.7852 - val_loss: 0.7077 - val_accuracy: 0.7962\n",
      "Epoch 64/300\n",
      "86/86 [==============================] - 1s 17ms/step - loss: 0.6294 - accuracy: 0.7955 - val_loss: 0.4434 - val_accuracy: 0.8490\n",
      "Epoch 65/300\n",
      "86/86 [==============================] - 2s 17ms/step - loss: 0.6170 - accuracy: 0.7925 - val_loss: 0.9991 - val_accuracy: 0.7185\n",
      "Epoch 66/300\n",
      "86/86 [==============================] - 1s 17ms/step - loss: 0.6320 - accuracy: 0.7903 - val_loss: 1.7067 - val_accuracy: 0.6305\n",
      "Epoch 67/300\n",
      "86/86 [==============================] - 1s 16ms/step - loss: 0.6077 - accuracy: 0.7955 - val_loss: 0.5364 - val_accuracy: 0.8226\n",
      "Epoch 68/300\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.6003 - accuracy: 0.7947 - val_loss: 0.4396 - val_accuracy: 0.8284\n",
      "Epoch 69/300\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.6361 - accuracy: 0.7859 - val_loss: 0.4602 - val_accuracy: 0.8402\n",
      "Epoch 70/300\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 0.5970 - accuracy: 0.8006 - val_loss: 0.4693 - val_accuracy: 0.8284\n",
      "Epoch 71/300\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.5947 - accuracy: 0.7947 - val_loss: 0.4439 - val_accuracy: 0.8328\n",
      "Epoch 72/300\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.5763 - accuracy: 0.7984 - val_loss: 0.5287 - val_accuracy: 0.8138\n",
      "Epoch 73/300\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.5818 - accuracy: 0.8017 - val_loss: 0.4269 - val_accuracy: 0.8387\n",
      "Epoch 74/300\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 0.5836 - accuracy: 0.8043 - val_loss: 0.5237 - val_accuracy: 0.8211\n",
      "Epoch 75/300\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 0.5593 - accuracy: 0.8112 - val_loss: 0.5533 - val_accuracy: 0.8314\n",
      "Epoch 76/300\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 0.5806 - accuracy: 0.8109 - val_loss: 0.5215 - val_accuracy: 0.8167\n",
      "Epoch 77/300\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.5906 - accuracy: 0.7988 - val_loss: 0.4061 - val_accuracy: 0.8622\n",
      "Epoch 78/300\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.5664 - accuracy: 0.8061 - val_loss: 0.5292 - val_accuracy: 0.8123\n",
      "Epoch 79/300\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 0.5697 - accuracy: 0.8087 - val_loss: 0.6985 - val_accuracy: 0.7786\n",
      "Epoch 80/300\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.5532 - accuracy: 0.8079 - val_loss: 1.2153 - val_accuracy: 0.6657\n",
      "Epoch 81/300\n",
      "86/86 [==============================] - 1s 16ms/step - loss: 0.5820 - accuracy: 0.7984 - val_loss: 1.4880 - val_accuracy: 0.6041\n",
      "Epoch 82/300\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.5627 - accuracy: 0.8024 - val_loss: 0.6980 - val_accuracy: 0.7918\n",
      "Epoch 83/300\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.5522 - accuracy: 0.8112 - val_loss: 0.5200 - val_accuracy: 0.8226\n",
      "Epoch 84/300\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.5723 - accuracy: 0.8061 - val_loss: 0.5247 - val_accuracy: 0.8240\n",
      "Epoch 85/300\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 0.5423 - accuracy: 0.8109 - val_loss: 0.5062 - val_accuracy: 0.8402\n",
      "Epoch 86/300\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 0.5444 - accuracy: 0.8105 - val_loss: 1.1911 - val_accuracy: 0.6862\n",
      "Epoch 87/300\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 0.5557 - accuracy: 0.8116 - val_loss: 0.4192 - val_accuracy: 0.8548\n",
      "Epoch 88/300\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.5260 - accuracy: 0.8244 - val_loss: 0.4479 - val_accuracy: 0.8490\n",
      "Epoch 89/300\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.5130 - accuracy: 0.8248 - val_loss: 0.4334 - val_accuracy: 0.8402\n",
      "Epoch 90/300\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.5206 - accuracy: 0.8163 - val_loss: 0.5365 - val_accuracy: 0.8196\n",
      "Epoch 91/300\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.5086 - accuracy: 0.8196 - val_loss: 1.3431 - val_accuracy: 0.6584\n",
      "Epoch 92/300\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.5318 - accuracy: 0.8130 - val_loss: 0.4606 - val_accuracy: 0.8328\n",
      "Epoch 93/300\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.5126 - accuracy: 0.8156 - val_loss: 0.5864 - val_accuracy: 0.8182\n",
      "Epoch 94/300\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 0.5055 - accuracy: 0.8240 - val_loss: 0.4304 - val_accuracy: 0.8636\n",
      "Epoch 95/300\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 0.5044 - accuracy: 0.8207 - val_loss: 1.3065 - val_accuracy: 0.6760\n",
      "Epoch 96/300\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 0.5065 - accuracy: 0.8226 - val_loss: 1.1178 - val_accuracy: 0.6965\n",
      "Epoch 97/300\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.5210 - accuracy: 0.8251 - val_loss: 0.4059 - val_accuracy: 0.8607\n",
      "Epoch 98/300\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.4995 - accuracy: 0.8207 - val_loss: 0.4428 - val_accuracy: 0.8475\n",
      "Epoch 99/300\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.5374 - accuracy: 0.8193 - val_loss: 0.4882 - val_accuracy: 0.8402\n",
      "Epoch 100/300\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.4877 - accuracy: 0.8270 - val_loss: 0.5036 - val_accuracy: 0.8255\n",
      "Epoch 101/300\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.4612 - accuracy: 0.8405 - val_loss: 0.3542 - val_accuracy: 0.8783\n",
      "Epoch 102/300\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.4785 - accuracy: 0.8343 - val_loss: 0.4103 - val_accuracy: 0.8490\n",
      "Epoch 103/300\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.4712 - accuracy: 0.8380 - val_loss: 0.7393 - val_accuracy: 0.7859\n",
      "Epoch 104/300\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.5110 - accuracy: 0.8149 - val_loss: 0.4113 - val_accuracy: 0.8548\n",
      "Epoch 105/300\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.4818 - accuracy: 0.8292 - val_loss: 0.5472 - val_accuracy: 0.8079\n",
      "Epoch 106/300\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.4621 - accuracy: 0.8383 - val_loss: 0.7887 - val_accuracy: 0.7683\n",
      "Epoch 107/300\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 0.4984 - accuracy: 0.8310 - val_loss: 0.4550 - val_accuracy: 0.8475\n",
      "Epoch 108/300\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.4574 - accuracy: 0.8354 - val_loss: 0.4527 - val_accuracy: 0.8402\n",
      "Epoch 109/300\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.4822 - accuracy: 0.8402 - val_loss: 0.6065 - val_accuracy: 0.8006\n",
      "Epoch 110/300\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.4418 - accuracy: 0.8431 - val_loss: 0.5922 - val_accuracy: 0.8065\n",
      "Epoch 111/300\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.4835 - accuracy: 0.8266 - val_loss: 0.3687 - val_accuracy: 0.8754\n",
      "Epoch 112/300\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.4637 - accuracy: 0.8405 - val_loss: 0.5560 - val_accuracy: 0.8152\n",
      "Epoch 113/300\n",
      "86/86 [==============================] - 1s 15ms/step - loss: 0.4973 - accuracy: 0.8343 - val_loss: 0.7304 - val_accuracy: 0.7757\n",
      "Epoch 114/300\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.5030 - accuracy: 0.8248 - val_loss: 0.5659 - val_accuracy: 0.8094\n",
      "Epoch 115/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 1s 12ms/step - loss: 0.4509 - accuracy: 0.8347 - val_loss: 0.3447 - val_accuracy: 0.8680\n",
      "Epoch 116/300\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.4455 - accuracy: 0.8435 - val_loss: 0.7935 - val_accuracy: 0.7625\n",
      "Epoch 117/300\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.4182 - accuracy: 0.8537 - val_loss: 0.9914 - val_accuracy: 0.7419\n",
      "Epoch 118/300\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.4560 - accuracy: 0.8409 - val_loss: 0.8938 - val_accuracy: 0.7229\n",
      "Epoch 119/300\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.4686 - accuracy: 0.8372 - val_loss: 0.5534 - val_accuracy: 0.8211\n",
      "Epoch 120/300\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.4615 - accuracy: 0.8405 - val_loss: 0.3864 - val_accuracy: 0.8607\n",
      "Epoch 121/300\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 0.4909 - accuracy: 0.8295 - val_loss: 0.3610 - val_accuracy: 0.8783\n",
      "Epoch 122/300\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.4952 - accuracy: 0.8354 - val_loss: 0.5281 - val_accuracy: 0.8211\n",
      "Epoch 123/300\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.4984 - accuracy: 0.8262 - val_loss: 0.4381 - val_accuracy: 0.8460\n",
      "Epoch 124/300\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.4709 - accuracy: 0.8446 - val_loss: 0.7516 - val_accuracy: 0.7639\n",
      "Epoch 125/300\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.4886 - accuracy: 0.8277 - val_loss: 0.4997 - val_accuracy: 0.8167\n",
      "Epoch 126/300\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.4703 - accuracy: 0.8387 - val_loss: 0.4100 - val_accuracy: 0.8387\n",
      "Epoch 127/300\n",
      "86/86 [==============================] - 2s 23ms/step - loss: 0.4623 - accuracy: 0.8383 - val_loss: 1.2126 - val_accuracy: 0.7009\n",
      "Epoch 128/300\n",
      "86/86 [==============================] - 1s 16ms/step - loss: 0.4535 - accuracy: 0.8358 - val_loss: 0.5124 - val_accuracy: 0.8211\n",
      "Epoch 129/300\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.4654 - accuracy: 0.8295 - val_loss: 0.3622 - val_accuracy: 0.8651\n",
      "Epoch 130/300\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.4296 - accuracy: 0.8493 - val_loss: 0.8302 - val_accuracy: 0.7786\n",
      "Epoch 131/300\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.4842 - accuracy: 0.8277 - val_loss: 0.7008 - val_accuracy: 0.7859\n",
      "Epoch 132/300\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.5100 - accuracy: 0.8218 - val_loss: 0.5100 - val_accuracy: 0.8240\n",
      "Epoch 133/300\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 0.4686 - accuracy: 0.8365 - val_loss: 0.3692 - val_accuracy: 0.8607\n",
      "Epoch 134/300\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.4544 - accuracy: 0.8424 - val_loss: 0.3777 - val_accuracy: 0.8607\n",
      "Epoch 135/300\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.4346 - accuracy: 0.8427 - val_loss: 0.4386 - val_accuracy: 0.8460\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fe772cd2d0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "import glob\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Get the current directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Specify the folder name where the images are located\n",
    "folder_name = \"img\"\n",
    "\n",
    "# Construct the folder path\n",
    "folder_path = os.path.join(current_directory, folder_name)\n",
    "\n",
    "# Specify the CSV file path containing image labels\n",
    "csv_file_path = os.path.join(current_directory, \"labels\", \"english-clean.csv\")\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Create a dictionary where keys are filenames and values are labels\n",
    "labels_dict = df.set_index('image')['label'].to_dict()\n",
    "\n",
    "def load_images_from_folder(folder, labels_dict):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for file_path in glob.glob(os.path.join(folder, \"*.png\")):\n",
    "        img = Image.open(file_path)\n",
    "        if img is not None:\n",
    "            img = img.convert('L')  # Convert image to grayscale\n",
    "            img = img.resize((28, 28))  # Resize the image\n",
    "            np_img = np.array(img)\n",
    "            images.append(np_img)\n",
    "            filename = os.path.basename(file_path)\n",
    "            label = labels_dict[filename]  # Get the label from the filename\n",
    "            labels.append(label)\n",
    "    return images, labels\n",
    "\n",
    "images, labels = load_images_from_folder(folder_path, labels_dict)\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "numerical_labels = le.fit_transform(labels)  # Convert labels to integers\n",
    "\n",
    "images = np.array(images)  # Convert list of arrays to a single array\n",
    "images = images / 255.0  # Normalize pixel values\n",
    "images = images.reshape(-1, 28, 28, 1)  # Reshape array for CNN\n",
    "\n",
    "encoded_labels = to_categorical(numerical_labels)  # One-hot encode labels\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, encoded_labels, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.25))  # Adjusted Dropout\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.25))  # Adjusted Dropout\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))  # Adjusted Dropout\n",
    "\n",
    "num_classes = len(set(numerical_labels))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Changed optimizer to SGD\n",
    "opt = SGD(learning_rate=0.01, momentum=0.9)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    zoom_range = 0.1,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1)\n",
    "\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "\n",
    "# Model Checkpoint\n",
    "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', save_best_only=True)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(datagen.flow(X_train, y_train, batch_size=32),\n",
    "          epochs=300,\n",
    "          validation_data=(X_test, y_test),\n",
    "          callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 0s 3ms/step - loss: 0.2503 - accuracy: 0.9054\n",
      "Train Loss: 0.2503087520599365\n",
      "Train Accuracy: 0.9054251909255981\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the training set\n",
    "train_loss, train_accuracy = model.evaluate(X_train, y_train)\n",
    "print(\"Train Loss:\", train_loss)\n",
    "print(\"Train Accuracy:\", train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 3ms/step - loss: 0.4386 - accuracy: 0.8460\n",
      "Test Loss: 0.4385736286640167\n",
      "Test Accuracy: 0.8460410833358765\n",
      "22/22 [==============================] - 0s 3ms/step\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.64      0.60        14\n",
      "           1       0.54      0.64      0.58        11\n",
      "           2       0.82      0.90      0.86        10\n",
      "           3       0.95      1.00      0.98        20\n",
      "           4       0.78      1.00      0.88        14\n",
      "           5       0.94      0.94      0.94        16\n",
      "           6       1.00      0.89      0.94         9\n",
      "           7       1.00      1.00      1.00        15\n",
      "           8       0.77      0.83      0.80        12\n",
      "           9       1.00      1.00      1.00        11\n",
      "          10       1.00      1.00      1.00        16\n",
      "          11       0.88      0.78      0.82         9\n",
      "          12       0.86      0.67      0.75         9\n",
      "          13       1.00      1.00      1.00        10\n",
      "          14       1.00      1.00      1.00        15\n",
      "          15       1.00      0.93      0.96        14\n",
      "          16       0.91      1.00      0.95        10\n",
      "          17       0.92      1.00      0.96        11\n",
      "          18       0.69      0.69      0.69        16\n",
      "          19       1.00      0.80      0.89         5\n",
      "          20       0.89      0.67      0.76        12\n",
      "          21       0.89      0.89      0.89         9\n",
      "          22       0.90      1.00      0.95         9\n",
      "          23       1.00      0.86      0.92         7\n",
      "          24       0.43      0.33      0.38         9\n",
      "          25       0.80      1.00      0.89         8\n",
      "          26       0.86      0.92      0.89        13\n",
      "          27       1.00      1.00      1.00         8\n",
      "          28       0.80      0.80      0.80        10\n",
      "          29       1.00      1.00      1.00        11\n",
      "          30       0.83      0.91      0.87        11\n",
      "          31       0.77      0.91      0.83        11\n",
      "          32       0.80      1.00      0.89        12\n",
      "          33       0.71      0.91      0.80        11\n",
      "          34       0.90      0.90      0.90        10\n",
      "          35       0.90      0.82      0.86        11\n",
      "          36       1.00      0.54      0.70        13\n",
      "          37       0.67      1.00      0.80         6\n",
      "          38       0.56      0.83      0.67         6\n",
      "          39       1.00      1.00      1.00         7\n",
      "          40       1.00      0.92      0.96        12\n",
      "          41       0.78      0.88      0.82         8\n",
      "          42       0.69      1.00      0.81        11\n",
      "          43       0.90      1.00      0.95         9\n",
      "          44       0.92      0.92      0.92        13\n",
      "          45       0.78      1.00      0.88         7\n",
      "          46       0.69      0.90      0.78        10\n",
      "          47       1.00      0.64      0.78        11\n",
      "          48       0.85      1.00      0.92        11\n",
      "          49       1.00      0.67      0.80        12\n",
      "          50       0.64      0.69      0.67        13\n",
      "          51       0.83      0.71      0.77         7\n",
      "          52       0.91      0.91      0.91        11\n",
      "          53       1.00      0.89      0.94         9\n",
      "          54       0.78      0.39      0.52        18\n",
      "          55       0.75      1.00      0.86        12\n",
      "          56       0.82      0.75      0.78        12\n",
      "          57       0.83      0.62      0.71         8\n",
      "          58       1.00      0.73      0.84        11\n",
      "          59       0.89      0.50      0.64        16\n",
      "          60       0.89      0.80      0.84        10\n",
      "          61       0.75      0.90      0.82        10\n",
      "\n",
      "    accuracy                           0.85       682\n",
      "   macro avg       0.85      0.85      0.84       682\n",
      "weighted avg       0.86      0.85      0.84       682\n",
      "\n",
      "Confusion Matrix:\n",
      "[[9 0 0 ... 0 0 0]\n",
      " [0 7 0 ... 0 0 0]\n",
      " [0 1 9 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 8 0 1]\n",
      " [0 0 0 ... 0 8 0]\n",
      " [0 0 0 ... 0 0 9]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "y_true_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true_labels, y_pred_labels, zero_division=1))\n",
    "\n",
    "# Print confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_mtx = confusion_matrix(y_true_labels, y_pred_labels)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_mtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 0s 3ms/step - loss: 3.6321 - accuracy: 0.0876\n",
      "Train Loss: 3.6320950984954834\n",
      "Train Accuracy: 0.08760996907949448\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the training set\n",
    "train_loss, train_accuracy = model.evaluate(X_train, y_train)\n",
    "print(\"Train Loss:\", train_loss)\n",
    "print(\"Train Accuracy:\", train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 2ms/step - loss: 2.0927 - accuracy: 0.7170\n",
      "Test Loss: 2.092695951461792\n",
      "Test Accuracy: 0.7170087695121765\n",
      "22/22 [==============================] - 0s 2ms/step\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.43      0.43        14\n",
      "           1       0.38      0.45      0.42        11\n",
      "           2       0.80      0.80      0.80        10\n",
      "           3       0.67      0.60      0.63        20\n",
      "           4       0.93      0.93      0.93        14\n",
      "           5       0.63      0.75      0.69        16\n",
      "           6       0.88      0.78      0.82         9\n",
      "           7       0.86      0.80      0.83        15\n",
      "           8       0.62      0.67      0.64        12\n",
      "           9       0.67      0.73      0.70        11\n",
      "          10       0.88      0.94      0.91        16\n",
      "          11       0.78      0.78      0.78         9\n",
      "          12       0.56      0.56      0.56         9\n",
      "          13       0.77      1.00      0.87        10\n",
      "          14       0.93      0.93      0.93        15\n",
      "          15       1.00      0.93      0.96        14\n",
      "          16       0.73      0.80      0.76        10\n",
      "          17       0.73      0.73      0.73        11\n",
      "          18       0.50      0.38      0.43        16\n",
      "          19       0.40      0.80      0.53         5\n",
      "          20       0.83      0.42      0.56        12\n",
      "          21       0.69      1.00      0.82         9\n",
      "          22       0.80      0.89      0.84         9\n",
      "          23       0.83      0.71      0.77         7\n",
      "          24       0.50      0.56      0.53         9\n",
      "          25       0.89      1.00      0.94         8\n",
      "          26       0.92      0.85      0.88        13\n",
      "          27       0.89      1.00      0.94         8\n",
      "          28       0.70      0.70      0.70        10\n",
      "          29       1.00      1.00      1.00        11\n",
      "          30       0.80      0.73      0.76        11\n",
      "          31       0.67      0.73      0.70        11\n",
      "          32       0.75      0.75      0.75        12\n",
      "          33       0.67      0.73      0.70        11\n",
      "          34       0.86      0.60      0.71        10\n",
      "          35       0.82      0.82      0.82        11\n",
      "          36       0.83      0.77      0.80        13\n",
      "          37       0.86      1.00      0.92         6\n",
      "          38       0.30      0.50      0.37         6\n",
      "          39       0.88      1.00      0.93         7\n",
      "          40       0.80      0.67      0.73        12\n",
      "          41       0.50      0.38      0.43         8\n",
      "          42       0.70      0.64      0.67        11\n",
      "          43       0.75      0.67      0.71         9\n",
      "          44       0.83      0.38      0.53        13\n",
      "          45       1.00      0.57      0.73         7\n",
      "          46       0.53      0.80      0.64        10\n",
      "          47       0.43      0.82      0.56        11\n",
      "          48       1.00      0.73      0.84        11\n",
      "          49       0.82      0.75      0.78        12\n",
      "          50       0.77      0.77      0.77        13\n",
      "          51       1.00      0.71      0.83         7\n",
      "          52       0.78      0.64      0.70        11\n",
      "          53       0.64      0.78      0.70         9\n",
      "          54       0.70      0.39      0.50        18\n",
      "          55       1.00      0.83      0.91        12\n",
      "          56       0.90      0.75      0.82        12\n",
      "          57       0.57      0.50      0.53         8\n",
      "          58       0.50      0.73      0.59        11\n",
      "          59       0.56      0.56      0.56        16\n",
      "          60       0.64      0.70      0.67        10\n",
      "          61       0.54      0.70      0.61        10\n",
      "\n",
      "    accuracy                           0.72       682\n",
      "   macro avg       0.73      0.73      0.72       682\n",
      "weighted avg       0.74      0.72      0.72       682\n",
      "\n",
      "Confusion Matrix:\n",
      "[[6 0 0 ... 0 0 0]\n",
      " [0 5 0 ... 0 0 1]\n",
      " [0 0 8 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 9 0 2]\n",
      " [0 0 0 ... 0 7 0]\n",
      " [0 0 0 ... 1 0 7]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "y_true_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true_labels, y_pred_labels, zero_division=1))\n",
    "\n",
    "# Print confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_mtx = confusion_matrix(y_true_labels, y_pred_labels)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_mtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>img001-001.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>img001-002.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>img001-003.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>img001-004.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>img001-005.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3405</th>\n",
       "      <td>img062-051.png</td>\n",
       "      <td>z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3406</th>\n",
       "      <td>img062-052.png</td>\n",
       "      <td>z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3407</th>\n",
       "      <td>img062-053.png</td>\n",
       "      <td>z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3408</th>\n",
       "      <td>img062-054.png</td>\n",
       "      <td>z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3409</th>\n",
       "      <td>img062-055.png</td>\n",
       "      <td>z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3410 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               image label\n",
       "0     img001-001.png     0\n",
       "1     img001-002.png     0\n",
       "2     img001-003.png     0\n",
       "3     img001-004.png     0\n",
       "4     img001-005.png     0\n",
       "...              ...   ...\n",
       "3405  img062-051.png     z\n",
       "3406  img062-052.png     z\n",
       "3407  img062-053.png     z\n",
       "3408  img062-054.png     z\n",
       "3409  img062-055.png     z\n",
       "\n",
       "[3410 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 2ms/step - loss: 1.3963 - accuracy: 0.5982\n",
      "Test Loss: 1.3962992429733276\n",
      "Test Accuracy: 0.5982404947280884\n",
      "22/22 [==============================] - 0s 2ms/step\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.36      0.40        14\n",
      "           1       0.26      0.82      0.40        11\n",
      "           2       0.78      0.70      0.74        10\n",
      "           3       0.67      0.70      0.68        20\n",
      "           4       0.59      0.71      0.65        14\n",
      "           5       0.75      0.56      0.64        16\n",
      "           6       0.50      0.56      0.53         9\n",
      "           7       0.45      0.87      0.59        15\n",
      "           8       0.50      0.67      0.57        12\n",
      "           9       0.58      0.64      0.61        11\n",
      "          10       0.93      0.81      0.87        16\n",
      "          11       1.00      0.44      0.62         9\n",
      "          12       0.58      0.78      0.67         9\n",
      "          13       0.90      0.90      0.90        10\n",
      "          14       0.86      0.80      0.83        15\n",
      "          15       0.81      0.93      0.87        14\n",
      "          16       0.62      1.00      0.77        10\n",
      "          17       0.67      0.36      0.47        11\n",
      "          18       0.50      0.06      0.11        16\n",
      "          19       0.25      0.20      0.22         5\n",
      "          20       0.53      0.67      0.59        12\n",
      "          21       0.62      0.56      0.59         9\n",
      "          22       0.73      0.89      0.80         9\n",
      "          23       0.71      0.71      0.71         7\n",
      "          24       0.38      0.33      0.35         9\n",
      "          25       0.86      0.75      0.80         8\n",
      "          26       1.00      0.31      0.47        13\n",
      "          27       0.80      0.50      0.62         8\n",
      "          28       0.62      0.50      0.56        10\n",
      "          29       0.75      0.82      0.78        11\n",
      "          30       0.73      0.73      0.73        11\n",
      "          31       0.80      0.73      0.76        11\n",
      "          32       0.64      0.75      0.69        12\n",
      "          33       0.78      0.64      0.70        11\n",
      "          34       0.67      0.60      0.63        10\n",
      "          35       0.69      0.82      0.75        11\n",
      "          36       0.50      0.69      0.58        13\n",
      "          37       0.71      0.83      0.77         6\n",
      "          38       0.33      0.83      0.48         6\n",
      "          39       1.00      0.71      0.83         7\n",
      "          40       0.88      0.58      0.70        12\n",
      "          41       0.56      0.62      0.59         8\n",
      "          42       0.60      0.27      0.37        11\n",
      "          43       0.62      0.56      0.59         9\n",
      "          44       0.36      0.62      0.46        13\n",
      "          45       0.43      0.86      0.57         7\n",
      "          46       1.00      0.00      0.00        10\n",
      "          47       1.00      0.18      0.31        11\n",
      "          48       0.80      0.73      0.76        11\n",
      "          49       0.89      0.67      0.76        12\n",
      "          50       0.59      0.77      0.67        13\n",
      "          51       1.00      0.29      0.44         7\n",
      "          52       0.56      0.45      0.50        11\n",
      "          53       0.38      0.67      0.48         9\n",
      "          54       0.64      0.39      0.48        18\n",
      "          55       0.50      0.25      0.33        12\n",
      "          56       1.00      0.58      0.74        12\n",
      "          57       0.29      0.25      0.27         8\n",
      "          58       0.50      0.45      0.48        11\n",
      "          59       0.57      0.50      0.53        16\n",
      "          60       0.38      0.50      0.43        10\n",
      "          61       0.39      0.70      0.50        10\n",
      "\n",
      "    accuracy                           0.60       682\n",
      "   macro avg       0.65      0.60      0.59       682\n",
      "weighted avg       0.66      0.60      0.59       682\n",
      "\n",
      "Confusion Matrix:\n",
      "[[5 0 0 ... 0 0 0]\n",
      " [0 9 0 ... 0 0 0]\n",
      " [0 0 7 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 8 0 6]\n",
      " [0 0 0 ... 0 5 0]\n",
      " [0 0 0 ... 0 0 7]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "y_true_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true_labels, y_pred_labels, zero_division=1))\n",
    "\n",
    "# Print confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_mtx = confusion_matrix(y_true_labels, y_pred_labels)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_mtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image Description](Image/Twitter1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import re\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    '''\n",
    "    Translate nltk POS to wordnet tags\n",
    "    '''\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def clean_text(text_list, tokenizer, stopwords_list, remove_words):\n",
    "    '''\n",
    "    Takes in a list of strings, a tokenizer, a list of stopwords, and a list of words to remove.\n",
    "    Returns a list of lowercased, tokenized, stopwords-removed, and lemmatized words.\n",
    "    '''\n",
    "    # lowercase\n",
    "    lower = [str(text).lower() for text in text_list]\n",
    "\n",
    "    # tokenize\n",
    "    tokenized = [tokenizer.tokenize(tweet) for tweet in lower]\n",
    "\n",
    "    # stopwords and special characters\n",
    "    no_stops = []\n",
    "    for item in tokenized:\n",
    "        temp = []\n",
    "        for token in item:\n",
    "            if token not in stopwords_list and token not in remove_words:\n",
    "                # Remove special characters\n",
    "                token = re.sub(r'\\W+', '', token)\n",
    "                temp.append(token)\n",
    "        no_stops.append(temp)\n",
    "\n",
    "    # preparation for lemmatization\n",
    "    tags = [pos_tag(tokens) for tokens in no_stops]\n",
    "\n",
    "    better_tags = []\n",
    "    for item in tags:\n",
    "        temp1 = []\n",
    "        for word in item:\n",
    "            temp1.append((word[0], get_wordnet_pos(word[1])))\n",
    "        better_tags.append(temp1)\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    lem = []\n",
    "    for item in better_tags:\n",
    "        temp2 = []\n",
    "        for word in item:\n",
    "            temp2.append(lemmatizer.lemmatize(word[0], word[1]))\n",
    "        lem.append(temp2)\n",
    "\n",
    "    preprocessed = [' '.join(i) for i in lem]\n",
    "\n",
    "    return preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "df = pd.read_csv('judge-1377884607_tweet_product_company.csv', encoding='latin1')\n",
    "\n",
    "# Remove tweets with unknown sentiment\n",
    "df = df[df['is_there_an_emotion_directed_at_a_brand_or_product'] != \"I can't tell\"]\n",
    "\n",
    "# Remove 'No emotion toward brand or product' category\n",
    "df = df[df['is_there_an_emotion_directed_at_a_brand_or_product'] != 'No emotion toward brand or product']\n",
    "\n",
    "# Load and preprocess the data\n",
    "X = df['tweet_text']\n",
    "y = df['is_there_an_emotion_directed_at_a_brand_or_product']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tokenizer and stopwords list\n",
    "tokenizer = TweetTokenizer()\n",
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "# Define the words to remove\n",
    "remove_words = ['rt', 'mention', '2', 'iphone', 'sxswi', '2', '522', 522, '32', '0134']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the pipeline for logistic regression\n",
    "lr_pipeline = Pipeline([\n",
    "    ('preprocessor', FunctionTransformer(clean_text, kw_args={'tokenizer': tokenizer, 'stopwords_list': stopwords_list, 'remove_words': remove_words})),\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('classifier', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# Create the pipeline for the dummy model\n",
    "dummy_pipeline = Pipeline([\n",
    "    ('preprocessor', FunctionTransformer(clean_text, kw_args={'tokenizer': tokenizer, 'stopwords_list': stopwords_list, 'remove_words': remove_words})),\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('classifier', DummyClassifier(strategy='most_frequent'))\n",
    "])\n",
    "\n",
    "# Define the hyperparameters to tune for logistic regression\n",
    "param_grid = {\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2)],  # unigrams or bigrams\n",
    "    'classifier__C': [0.1, 1.0, 10.0]  # inverse of regularization strength\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform hyperparameter tuning using GridSearchCV for logistic regression\n",
    "grid_search_lr = GridSearchCV(lr_pipeline, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search_lr.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters and model from GridSearchCV for logistic regression\n",
    "best_params_lr = grid_search_lr.best_params_\n",
    "best_model_lr = grid_search_lr.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the logistic regression model with the best hyperparameters\n",
    "best_model_lr.fit(X_train, y_train)\n",
    "\n",
    "# Train the dummy model\n",
    "dummy_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions using the logistic regression model\n",
    "y_pred_lr = best_model_lr.predict(X_test)\n",
    "\n",
    "# Make predictions using the dummy model\n",
    "y_pred_dummy = dummy_pipeline.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics for the logistic regression model\n",
    "accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
    "precision_lr = precision_score(y_test, y_pred_lr, average='weighted', zero_division=1.0)\n",
    "recall_lr = recall_score(y_test, y_pred_lr, average='weighted', zero_division=1.0)\n",
    "f1_lr = f1_score(y_test, y_pred_lr, average='weighted', zero_division=1.0)\n",
    "\n",
    "# Calculate evaluation metrics for the dummy model\n",
    "accuracy_dummy = accuracy_score(y_test, y_pred_dummy)\n",
    "precision_dummy = precision_score(y_test, y_pred_dummy, average='weighted', zero_division=1.0)\n",
    "recall_dummy = recall_score(y_test, y_pred_dummy, average='weighted', zero_division=1.0)\n",
    "f1_dummy = f1_score(y_test, y_pred_dummy, average='weighted', zero_division=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the best hyperparameters\n",
    "print('Best Hyperparameters:')\n",
    "for param, value in best_params_lr.items():\n",
    "    print(f'{param}: {value}')\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the evaluation metrics for the logistic regression model\n",
    "print('Logistic Regression Model:')\n",
    "print(f'Accuracy: {accuracy_lr}')\n",
    "print(f'Precision: {precision_lr}')\n",
    "print(f'Recall: {recall_lr}')\n",
    "print(f'F1-score: {f1_lr}')\n",
    "print('')\n",
    "\n",
    "# Print the evaluation metrics for the dummy model\n",
    "print('Dummy Classifier:')\n",
    "print(f'Accuracy: {accuracy_dummy}')\n",
    "print(f'Precision: {precision_dummy}')\n",
    "print(f'Recall: {recall_dummy}')\n",
    "print(f'F1-score: {f1_dummy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the class labels\n",
    "class_labels = best_model_lr.named_steps['classifier'].classes_\n",
    "\n",
    "# Print the class labels\n",
    "for i, label in enumerate(class_labels):\n",
    "    print(f\"Class {i}: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the class labels\n",
    "class_labels = best_model_lr.named_steps['classifier'].classes_\n",
    "\n",
    "# Print the class labels\n",
    "for i, label in enumerate(class_labels):\n",
    "    print(f\"Class {i}: {label}\")\n",
    "\n",
    "# Perform feature analysis for logistic regression\n",
    "if 'tfidf' in best_model_lr.named_steps:\n",
    "    tfidf_vec = best_model_lr.named_steps['tfidf']\n",
    "    feature_names = tfidf_vec.get_feature_names_out()\n",
    "\n",
    "    # Get the coefficients for the positive class\n",
    "    coefficients = best_model_lr.named_steps['classifier'].coef_[0]\n",
    "\n",
    "    # Sort the feature names and coefficients\n",
    "    sorted_indices = coefficients.argsort()\n",
    "    top_features = [feature_names[idx] for idx in sorted_indices[:10]]\n",
    "    bottom_features = [feature_names[idx] for idx in sorted_indices[-10:]]\n",
    "\n",
    "    # Print the top and bottom features\n",
    "    print('Negative emotion:')\n",
    "    print(top_features)\n",
    "    print('Positive emotion:')\n",
    "    print(bottom_features)\n",
    "    \n",
    "\n",
    "else:\n",
    "    print('Feature analysis is not available for the chosen model.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the class labels\n",
    "class_labels = best_model_lr.named_steps['classifier'].classes_\n",
    "\n",
    "# Print the class labels\n",
    "for i, label in enumerate(class_labels):\n",
    "    print(f\"Class {i}: {label}\")\n",
    "\n",
    "# Perform feature analysis for logistic regression\n",
    "if 'tfidf' in best_model_lr.named_steps:\n",
    "    tfidf_vec = best_model_lr.named_steps['tfidf']\n",
    "    feature_names = tfidf_vec.get_feature_names_out()\n",
    "\n",
    "    # Get the coefficients for the positive class\n",
    "    coefficients = best_model_lr.named_steps['classifier'].coef_[0]\n",
    "\n",
    "    # Sort the feature names and coefficients\n",
    "    sorted_indices = coefficients.argsort()\n",
    "    top_features = [feature_names[idx] for idx in sorted_indices[:10]]\n",
    "    bottom_features = [feature_names[idx] for idx in sorted_indices[-10:]]\n",
    "\n",
    "    # Print the top and bottom features\n",
    "    print('Negative emotion:')\n",
    "    print(top_features)\n",
    "    print('Positive emotion:')\n",
    "    print(bottom_features)\n",
    "\n",
    "    # Plotting the feature analysis\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(range(len(top_features)), coefficients[sorted_indices[:10]], align='center', color='#1DA1F2')\n",
    "    plt.yticks(range(len(top_features)), top_features)\n",
    "    plt.xlabel('Coefficient')\n",
    "    plt.ylabel('Top Features')\n",
    "    plt.title('Feature Analysis for Negative Emotion')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(range(len(bottom_features)), coefficients[sorted_indices[-10:]], align='center', color='#1DA1F2')\n",
    "    plt.yticks(range(len(bottom_features)), bottom_features)\n",
    "    plt.xlabel('Coefficient')\n",
    "    plt.ylabel('Positive emotion')\n",
    "    plt.title('Feature Analysis for Positive Emotion')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('Feature analysis is not available for the chosen model.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Extract the coefficients and feature names\n",
    "coefficients = best_model_lr.named_steps['classifier'].coef_[0]\n",
    "feature_names = best_model_lr.named_steps['tfidf'].get_feature_names_out()\n",
    "\n",
    "# Create a DataFrame for positive emotion\n",
    "positive_df = pd.DataFrame({'Token': feature_names, 'Value Impact': coefficients})\n",
    "positive_df = positive_df.sort_values(by='Value Impact', ascending=False)\n",
    "\n",
    "# Create a DataFrame for negative emotion\n",
    "negative_df = pd.DataFrame({'Token': feature_names, 'Value Impact': -coefficients})\n",
    "negative_df = negative_df.sort_values(by='Value Impact', ascending=False)\n",
    "\n",
    "# Print the positive DataFrame\n",
    "print('Positive Emotion DataFrame:')\n",
    "print(positive_df.head(50))\n",
    "print()\n",
    "\n",
    "# Print the negative DataFrame\n",
    "print('Negative Emotion DataFrame:')\n",
    "print(negative_df.head(50))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "negative_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the positive dataframe to CSV\n",
    "positive_df.to_csv('positive_emotion_dataframe.csv', index=True)\n",
    "\n",
    "# Save the negative dataframe to CSV\n",
    "negative_df.to_csv('negative_emotion_dataframe.csv', index=True)\n",
    "\n",
    "print('Dataframes saved to CSV files.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Clean the text data\n",
    "X_cleaned = clean_text(X, tokenizer, stopwords_list, remove_words)\n",
    "\n",
    "# Tokenize the cleaned text\n",
    "tokens = [token for tweet_tokens in X_cleaned for token in tokenizer.tokenize(tweet_tokens)]\n",
    "\n",
    "# Remove the words specified for removal\n",
    "tokens_filtered = [token for token in tokens if token not in remove_words]\n",
    "\n",
    "# Count the frequency of each word\n",
    "word_counts = Counter(tokens_filtered)\n",
    "\n",
    "# Get the top 10 most common words and their frequencies\n",
    "top_words = word_counts.most_common(30)\n",
    "\n",
    "# Extract the words\n",
    "words = [word for word, _ in top_words]\n",
    "\n",
    "# Print the top 10 words\n",
    "print(\"Top 30 Words:\")\n",
    "for word in words:\n",
    "    print(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Top 10 words and their frequencies\n",
    "top_words = word_counts.most_common(30)\n",
    "words, frequencies = zip(*top_words)\n",
    "\n",
    "# Plot the bar graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(words, frequencies, color='#1DA1F2')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Top 30 Words')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Clean the text data\n",
    "X_cleaned = clean_text(X, tokenizer, stopwords_list, remove_words)\n",
    "\n",
    "# Tokenize the cleaned text\n",
    "tokens = [token for tweet_tokens in X_cleaned for token in tokenizer.tokenize(tweet_tokens)]\n",
    "\n",
    "# Remove the words specified for removal\n",
    "tokens_filtered = [token for token in tokens if token not in remove_words]\n",
    "\n",
    "# Create a frequency dictionary of the filtered words\n",
    "word_freq = {}\n",
    "for word in tokens_filtered:\n",
    "    word_freq[word] = word_freq.get(word, 0) + 1\n",
    "\n",
    "# Generate the word cloud based on the word frequency\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)\n",
    "\n",
    "# Display the word cloud\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the coefficients for the positive class\n",
    "coefficients = best_model_lr.named_steps['classifier'].coef_[0]\n",
    "\n",
    "# Get the top and bottom features\n",
    "sorted_indices = coefficients.argsort()\n",
    "top_features = [feature_names[idx] for idx in sorted_indices[:50]]\n",
    "bottom_features = [feature_names[idx] for idx in sorted_indices[-50:]]\n",
    "\n",
    "# Join the word lists for positive and negative emotions\n",
    "negative_text = ' '.join(top_features)\n",
    "positive_text = ' '.join(bottom_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Function to generate word cloud with custom color\n",
    "def generate_word_cloud_with_color(text_data, title):\n",
    "    # Define the color function\n",
    "    def blue_color_func(word, font_size, position, orientation, random_state=None, **kwargs):\n",
    "        return \"hsl(200, 100%%, %d%%)\" % np.random.randint(30, 70)\n",
    "\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white', color_func=blue_color_func).generate(text_data)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Generate word cloud for positive emotion with shades of blue\n",
    "generate_word_cloud_with_color(positive_text, 'Word Cloud - Positive Emotion')\n",
    "generate_word_cloud_with_color(negative_text, 'Word Cloud - Negative Emotion')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Function to generate word cloud with custom color\n",
    "def generate_word_cloud_with_color(text_data, title):\n",
    "    # Define the color function for blue\n",
    "    def blue_color_func(word, font_size, position, orientation, random_state=None, **kwargs):\n",
    "        return \"hsl(200, 100%%, %d%%)\" % np.random.randint(30, 70)\n",
    "\n",
    "    # Define the color function for red\n",
    "    def red_color_func(word, font_size, position, orientation, random_state=None, **kwargs):\n",
    "        return \"hsl(0, 100%%, %d%%)\" % np.random.randint(30, 70)\n",
    "\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white')\n",
    "\n",
    "    if \"Positive\" in title:\n",
    "        wordcloud.generate_from_text(text_data)\n",
    "        color_func = blue_color_func\n",
    "    elif \"Negative\" in title:\n",
    "        wordcloud.generate_from_text(text_data)\n",
    "        color_func = red_color_func\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(wordcloud.recolor(color_func=color_func), interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Generate word cloud for positive emotion with shades of blue\n",
    "generate_word_cloud_with_color(positive_text, 'Word Cloud - Positive Emotion')\n",
    "# generate_word_cloud_with_color(negative_text, 'Word Cloud - Negative Emotion')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_word_cloud_with_color(negative_text, 'Word Cloud - Negative Emotion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Get the predicted labels for the test set\n",
    "y_pred = best_model_lr.predict(X_test)\n",
    "\n",
    "# Create the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Get the class labels\n",
    "class_labels = best_model_lr.named_steps['classifier'].classes_\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Get the predicted probabilities for each class\n",
    "y_probs = best_model_lr.predict_proba(X_test)\n",
    "\n",
    "# Compute the false positive rate (fpr), true positive rate (tpr), and thresholds for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "for i, class_label in enumerate(best_model_lr.named_steps['classifier'].classes_):\n",
    "    fpr[class_label], tpr[class_label], _ = roc_curve(y_test, y_probs[:, i], pos_label=class_label)\n",
    "    roc_auc[class_label] = auc(fpr[class_label], tpr[class_label])\n",
    "\n",
    "# Plot the AUC-ROC curve for each class\n",
    "plt.figure(figsize=(8, 6))\n",
    "for class_label in best_model_lr.named_steps['classifier'].classes_:\n",
    "    plt.plot(fpr[class_label], tpr[class_label], label=f'{class_label} (AUC = {roc_auc[class_label]:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('AUC-ROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image Description](Image/Twitter.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfgpu",
   "language": "python",
   "name": "tfgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
